{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Word2Vec, Tokenizer, StandardScaler\n",
    "from pyspark.sql.functions import regexp_replace, lower, col, udf, asc, desc, explode\n",
    "from pyspark.sql.functions import split, size, array_join\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "from pyspark.ml.classification import RandomForestClassifier, NaiveBayes, LogisticRegression\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.feature import StringIndexer, Tokenizer, HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gensim import corpora, models\n",
    "import numpy as np\n",
    "\n",
    "from operator import contains\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# Needed on RaaS\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "\n",
    "# Not needed\n",
    "# os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# cred ca vrea sa fie rulat totusi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_spark():\n",
    "  spark = SparkSession.builder.config(\"spark.driver.memory\", \"5g\").\\\n",
    "                              appName(\"BigData\").getOrCreate()\n",
    "  \n",
    "  sc = spark.sparkContext\n",
    "  return spark, sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess dataset for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_from_file(file_name, output_file, sample_size = 10_000):\n",
    "    \"\"\"\n",
    "    Reads a file and extracts a sample of lines from it.\n",
    "    \n",
    "    Parameters:\n",
    "    file_name (str): The name of the input file.\n",
    "    output_file (str): The name of the output file where the sample will be written.\n",
    "    sample_size (int): The number of lines to extract from the input file. Default is 10,000.\n",
    "    \"\"\"\n",
    "    \n",
    "    content = \"\"\n",
    "    with open(file_name, \"r\") as f:\n",
    "        for i in range(sample_size):\n",
    "            content += f.readline()\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_json(spark, file_name):\n",
    "    return spark.read.json(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Context and Load Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a sample from the input file\n",
    "get_sample_from_file(\"data.json\", \"arxiv-sample.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = init_spark()[0]\n",
    "arxiv_dataset = load_data_from_json(sc, \"arxiv-sample.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searcing for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for missing values\n",
    "print(\"Number of missing values in abstract column: \", arxiv_dataset.filter(arxiv_dataset.abstract.isNull()).count())\n",
    "print(\"Number of missing values in title column: \", arxiv_dataset.filter(arxiv_dataset.title.isNull()).count())\n",
    "print(\"Number of missing values in categories column: \", arxiv_dataset.filter(arxiv_dataset.categories.isNull()).count())\n",
    "print(\"Number of missing values in id column: \", arxiv_dataset.filter(arxiv_dataset.id.isNull()).count())\n",
    "print(\"Number of missing values in submitter column: \", arxiv_dataset.filter(arxiv_dataset.submitter.isNull()).count())\n",
    "print(\"Number of missing values in authors column: \", arxiv_dataset.filter(arxiv_dataset.authors.isNull()).count())\n",
    "print(\"Number of missing values in report-no column: \",arxiv_dataset.filter(arxiv_dataset[\"report-no\"].isNull()).count() )\n",
    "print(\"Number of missing values in comments column: \", arxiv_dataset.filter(arxiv_dataset.comments.isNull()).count())\n",
    "print(\"Number of missing values in doi column: \", arxiv_dataset.filter(arxiv_dataset.doi.isNull()).count())\n",
    "print(\"Number of missing values in journal-ref column: \", arxiv_dataset.filter(arxiv_dataset[\"journal-ref\"].isNull()).count())\n",
    "print(\"Number of missing values in versions column: \", arxiv_dataset.filter(arxiv_dataset.versions.isNull()).count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show rows with empty fields\n",
    "arxiv_dataset.filter(arxiv_dataset[\"abstract\"] == \"\").count()\n",
    "arxiv_dataset.filter(arxiv_dataset[\"title\"] == \"\").count()\n",
    "arxiv_dataset.filter(arxiv_dataset[\"authors\"] == \"\").count()\n",
    "arxiv_dataset.filter(arxiv_dataset[\"categories\"] == \"\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill ALL NULL values with empty string : use after removing rows with empty fields\n",
    "def fill_na_with_empty_string(df):\n",
    "    return df.fillna(\"\")\n",
    "\n",
    "# remove rows with empty fields\n",
    "def remove_empty_fields(df, field_name):\n",
    "    return df.filter(df[field_name] != \"\")\n",
    "\n",
    "# remove \"\\n\" from text\n",
    "def remove_empty_newlines(df, field_name):\n",
    "    return df.withColumn(field_name, regexp_replace(col(field_name), \"\\n\", \" \"))\n",
    "\n",
    "# remove math formulas and latex\n",
    "def remove_math_formula(df, field_name):\n",
    "    return df.withColumn(field_name, regexp_replace(col(field_name), \"\\$.*?\\$\", \"\"))\n",
    "\n",
    "# convert to lowercase\n",
    "def convert_to_lowercase(df, field_name):\n",
    "    return df.withColumn(field_name, lower(col(field_name)))\n",
    "\n",
    "# remove extra spaces\n",
    "def remove_extra_spaces(df, field_name):\n",
    "    df = df.withColumn(field_name, regexp_replace(col(field_name), \" +\", \" \"))\n",
    "    df = df.withColumn(field_name, regexp_replace(col(field_name), \"^ +\", \"\"))\n",
    "    return df\n",
    "\n",
    "def remove_punctuation_except_dot(df, field_name):\n",
    "    pattern = \"[^a-zA-Z0-9\\s\\.,]\"\n",
    "    cleaned_df = df.withColumn(field_name, regexp_replace(col(field_name), pattern, \"\"))\n",
    "    return cleaned_df\n",
    "\n",
    "# remove punctuation\n",
    "def remove_punctuation(df, field_name):\n",
    "    return df.withColumn(field_name, regexp_replace(col(field_name), \"[^\\w\\s]\", \"\"))\n",
    "\n",
    "def remove_text_in_braces(df, field_name):\n",
    "    pattern = r'[\\(\\{\\[].*?[\\)\\}\\]]'\n",
    "    cleaned_df = df.withColumn(field_name, regexp_replace(col(field_name), pattern, ''))\n",
    "    return cleaned_df\n",
    "\n",
    "def remove_text_between_parentheses(strings_list):\n",
    "    result_list = []\n",
    "    for string in strings_list:\n",
    "        modified_string = re.sub(r'\\([^()]*\\)', '', string)\n",
    "        result_list.append(modified_string)\n",
    "    return result_list\n",
    "\n",
    "def remove_substring_from_list(input_list, substring_to_remove):\n",
    "    return [element.replace(substring_to_remove, \"\") for element in input_list]\n",
    "\n",
    "def remove_stop_words(df, field_name, output_field, stop_words):\n",
    "  df = df.withColumn(\"tokens\", split(field_name, \"\\\\s+\"))\n",
    "  remover = StopWordsRemover(stopWords=stop_words, inputCol=\"tokens\", outputCol=output_field)\n",
    "  return remover.transform(df).select(field_name, \"categories\", array_join(output_field, \" \").alias(output_field))\n",
    "\n",
    "def split_function(sep, authors_list):\n",
    "  for i in range(len(authors_list)):\n",
    "      element = authors_list[i]\n",
    "      author = element.split(sep)\n",
    "      authors_list[i] = author\n",
    "  authors_list = set([item for sublist in authors_list for item in sublist])\n",
    "  return list(authors_list)\n",
    "\n",
    "def delete_new_line_and_spaces(author_list):\n",
    "  for i in range(len(author_list)):\n",
    "    if author_list[i].startswith(\"\\n \"):\n",
    "      author_list[i] = author_list[i].replace(\"\\n \", '', 1)\n",
    "  for i in range(len(author_list)):\n",
    "    author_list[i] = author_list[i].strip()\n",
    "  for author in author_list:\n",
    "    if author == '':\n",
    "      author_list.remove(author)\n",
    "  return author_list\n",
    "\n",
    "def delete_duplicate(author_list):\n",
    "  distinct_list = []\n",
    "  for author in author_list:\n",
    "    if author not in distinct_list:\n",
    "      distinct_list.append(author)\n",
    "  return distinct_list\n",
    "\n",
    "def delete_letter(author_list):\n",
    "  for author in author_list:\n",
    "    if len(author) == 1:\n",
    "      author_list.remove(author)\n",
    "    elif len(author) == 2 and author[-1] == \".\":\n",
    "      author_list.remove(author)\n",
    "  return author_list\n",
    "\n",
    "def remove_characters_before_substring(original_list, target_substring):\n",
    "    return [remove_characters(element, target_substring) for element in original_list]\n",
    "\n",
    "def remove_characters(original_string, target_substring):\n",
    "    index = original_string.lower().find(target_substring.lower())\n",
    "    if index != -1:\n",
    "        return original_string[index + len(target_substring):]\n",
    "    else:\n",
    "        return original_string\n",
    "\n",
    "def delete_element(input_list, substr):\n",
    "  for element in input_list:\n",
    "    if substr.lower() in element.lower():\n",
    "      input_list.remove(element)\n",
    "  return input_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clear 'abstract' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_dataset = remove_empty_fields(arxiv_dataset, \"abstract\")\n",
    "arxiv_dataset = remove_empty_newlines(arxiv_dataset, \"abstract\")\n",
    "arxiv_dataset = remove_math_formula(arxiv_dataset, \"abstract\")\n",
    "arxiv_dataset = convert_to_lowercase(arxiv_dataset, \"abstract\")\n",
    "arxiv_dataset = remove_extra_spaces(arxiv_dataset, \"abstract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some changed rows\n",
    "arxiv_dataset.select(\"abstract\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Abstract to Vector Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Word2Vec model\n",
    "input_col = \"abstract\"\n",
    "output_col = \"abstract_vector\"\n",
    "\n",
    "words2vec_model = Word2Vec(\n",
    "    inputCol=\"words\",\n",
    "    outputCol=output_col,\n",
    "    vectorSize=100,\n",
    "    minCount=5\n",
    ")\n",
    "\n",
    "tokenized = Tokenizer(inputCol=input_col, outputCol=\"words\")\n",
    "tokenized_dataset = tokenized.transform(arxiv_dataset)\n",
    "model = words2vec_model.fit(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.getVectors().show(n=2, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vizualize most popular categories based on how many authors contributed to that category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading back the data\n",
    "arxiv_dataset = load_data_from_json(sc, \"arxiv-sample.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=ArrayType(StringType()))\n",
    "def get_authors_list(parsed_authors):\n",
    "    authors = [\n",
    "        f\"{author[0]} {author[1]}\" for author in parsed_authors\n",
    "    ]\n",
    "    return authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_dataset = arxiv_dataset.withColumn(\"authors_list\", get_authors_list(\"authors_parsed\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_dataset.select(\"authors_list\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess authors column : remove the \"and\" word and split the authors by comma\n",
    "def preprocess_authors(df):\n",
    "    return df.withColumn(\"authors\", regexp_replace(col(\"authors\"), \" and \", \", \"))\n",
    "\n",
    "# preprocess authors column : remove extra commas\n",
    "def remove_authors_extra_commas(df):\n",
    "    return df.withColumn(\"authors\", regexp_replace(col(\"authors\"), \",+\", \",\"))\n",
    "    \n",
    "# preprocess authors column : remove empty spaces (more than one space)\n",
    "def remove_authors_extra_spaces(df):\n",
    "    return df.withColumn(\"authors\", regexp_replace(col(\"authors\"), \" +\", \" \"))\n",
    "\n",
    "# preprocess authors column : remove parentheses and their content\n",
    "def remove_authors_parentheses(df):\n",
    "    return df.withColumn(\"authors\", regexp_replace(col(\"authors\"), \"\\(.+?\\)\", \"\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_dataset = preprocess_authors(arxiv_dataset)\n",
    "arxiv_dataset = remove_authors_extra_commas(arxiv_dataset)\n",
    "arxiv_dataset = remove_authors_extra_spaces(arxiv_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the first 5 rows\n",
    "arxiv_dataset.select(\"authors_list\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all categories\n",
    "categories = arxiv_dataset.select(\"categories\")\n",
    "\n",
    "categories = (\n",
    "    categories.rdd.map(lambda x: x.categories)\n",
    "    .map(lambda x: x.split(\" \"))\n",
    "    .map(lambda x: [item for item in x])\n",
    "    .flatMap(lambda x: x)\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "print(\"Number of categories: \", categories.count())\n",
    "\n",
    "# Get all authors\n",
    "authors = arxiv_dataset.select(\"authors_list\")\n",
    "authors = (\n",
    "    authors.rdd.map(lambda x: x.authors_list)\n",
    "    .map(lambda x: [item for item in x])\n",
    "    .flatMap(lambda x: x)\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "print(\"Number of authors: \", authors.count())\n",
    "\n",
    "# get all authors for each category\n",
    "\n",
    "categories_authors = {}\n",
    "auth_categories = arxiv_dataset.select(\"categories\", \"authors_list\")\n",
    "categories_authors = (\n",
    "    auth_categories.rdd.map(lambda x: (x.categories, x.authors_list))\n",
    "    .map(lambda x: (x[0].split(\" \"), x[1]))\n",
    "    .flatMap(lambda x: [(item, x[1]) for item in x[0]])\n",
    "    .reduceByKey(lambda x, y: x + y)\n",
    "    .map(lambda x: (x[0], list(set(x[1]))))\n",
    ")\n",
    "\n",
    "# categories_authors to dict\n",
    "categories_authors = dict(categories_authors.collect())\n",
    "\n",
    "# get number of authors for each category\n",
    "count_authors = {}\n",
    "for category in categories_authors:\n",
    "    count_authors[category] = len(categories_authors[category])\n",
    "\n",
    "# sort categories based on number of authors\n",
    "sorted_categories = sorted(count_authors.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# get top 10 categories\n",
    "top_categories = sorted_categories[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic extraction and stats (most popular topic per category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERTopic? Gensim? LDA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading back the data\n",
    "arxiv_dataset = load_data_from_json(sc, \"arxiv-sample.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://arxiv.org/help/api/user-manual\n",
    "category_map = {'astro-ph': 'Astrophysics',\n",
    "'astro-ph.CO': 'Cosmology and Nongalactic Astrophysics',\n",
    "'astro-ph.EP': 'Earth and Planetary Astrophysics',\n",
    "'astro-ph.GA': 'Astrophysics of Galaxies',\n",
    "'astro-ph.HE': 'High Energy Astrophysical Phenomena',\n",
    "'astro-ph.IM': 'Instrumentation and Methods for Astrophysics',\n",
    "'astro-ph.SR': 'Solar and Stellar Astrophysics',\n",
    "'cond-mat.dis-nn': 'Disordered Systems and Neural Networks',\n",
    "'cond-mat.mes-hall': 'Mesoscale and Nanoscale Physics',\n",
    "'cond-mat.mtrl-sci': 'Materials Science',\n",
    "'cond-mat.other': 'Other Condensed Matter',\n",
    "'cond-mat.quant-gas': 'Quantum Gases',\n",
    "'cond-mat.soft': 'Soft Condensed Matter',\n",
    "'cond-mat.stat-mech': 'Statistical Mechanics',\n",
    "'cond-mat.str-el': 'Strongly Correlated Electrons',\n",
    "'cond-mat.supr-con': 'Superconductivity',\n",
    "'cs.AI': 'Artificial Intelligence',\n",
    "'cs.AR': 'Hardware Architecture',\n",
    "'cs.CC': 'Computational Complexity',\n",
    "'cs.CE': 'Computational Engineering, Finance, and Science',\n",
    "'cs.CG': 'Computational Geometry',\n",
    "'cs.CL': 'Computation and Language',\n",
    "'cs.CR': 'Cryptography and Security',\n",
    "'cs.CV': 'Computer Vision and Pattern Recognition',\n",
    "'cs.CY': 'Computers and Society',\n",
    "'cs.DB': 'Databases',\n",
    "'cs.DC': 'Distributed, Parallel, and Cluster Computing',\n",
    "'cs.DL': 'Digital Libraries',\n",
    "'cs.DM': 'Discrete Mathematics',\n",
    "'cs.DS': 'Data Structures and Algorithms',\n",
    "'cs.ET': 'Emerging Technologies',\n",
    "'cs.FL': 'Formal Languages and Automata Theory',\n",
    "'cs.GL': 'General Literature',\n",
    "'cs.GR': 'Graphics',\n",
    "'cs.GT': 'Computer Science and Game Theory',\n",
    "'cs.HC': 'Human-Computer Interaction',\n",
    "'cs.IR': 'Information Retrieval',\n",
    "'cs.IT': 'Information Theory',\n",
    "'cs.LG': 'Machine Learning',\n",
    "'cs.LO': 'Logic in Computer Science',\n",
    "'cs.MA': 'Multiagent Systems',\n",
    "'cs.MM': 'Multimedia',\n",
    "'cs.MS': 'Mathematical Software',\n",
    "'cs.NA': 'Numerical Analysis',\n",
    "'cs.NE': 'Neural and Evolutionary Computing',\n",
    "'cs.NI': 'Networking and Internet Architecture',\n",
    "'cs.OH': 'Other Computer Science',\n",
    "'cs.OS': 'Operating Systems',\n",
    "'cs.PF': 'Performance',\n",
    "'cs.PL': 'Programming Languages',\n",
    "'cs.RO': 'Robotics',\n",
    "'cs.SC': 'Symbolic Computation',\n",
    "'cs.SD': 'Sound',\n",
    "'cs.SE': 'Software Engineering',\n",
    "'cs.SI': 'Social and Information Networks',\n",
    "'cs.SY': 'Systems and Control',\n",
    "'econ.EM': 'Econometrics',\n",
    "'eess.AS': 'Audio and Speech Processing',\n",
    "'eess.IV': 'Image and Video Processing',\n",
    "'eess.SP': 'Signal Processing',\n",
    "'gr-qc': 'General Relativity and Quantum Cosmology',\n",
    "'hep-ex': 'High Energy Physics - Experiment',\n",
    "'hep-lat': 'High Energy Physics - Lattice',\n",
    "'hep-ph': 'High Energy Physics - Phenomenology',\n",
    "'hep-th': 'High Energy Physics - Theory',\n",
    "'math.AC': 'Commutative Algebra',\n",
    "'math.AG': 'Algebraic Geometry',\n",
    "'math.AP': 'Analysis of PDEs',\n",
    "'math.AT': 'Algebraic Topology',\n",
    "'math.CA': 'Classical Analysis and ODEs',\n",
    "'math.CO': 'Combinatorics',\n",
    "'math.CT': 'Category Theory',\n",
    "'math.CV': 'Complex Variables',\n",
    "'math.DG': 'Differential Geometry',\n",
    "'math.DS': 'Dynamical Systems',\n",
    "'math.FA': 'Functional Analysis',\n",
    "'math.GM': 'General Mathematics',\n",
    "'math.GN': 'General Topology',\n",
    "'math.GR': 'Group Theory',\n",
    "'math.GT': 'Geometric Topology',\n",
    "'math.HO': 'History and Overview',\n",
    "'math.IT': 'Information Theory',\n",
    "'math.KT': 'K-Theory and Homology',\n",
    "'math.LO': 'Logic',\n",
    "'math.MG': 'Metric Geometry',\n",
    "'math.MP': 'Mathematical Physics',\n",
    "'math.NA': 'Numerical Analysis',\n",
    "'math.NT': 'Number Theory',\n",
    "'math.OA': 'Operator Algebras',\n",
    "'math.OC': 'Optimization and Control',\n",
    "'math.PR': 'Probability',\n",
    "'math.QA': 'Quantum Algebra',\n",
    "'math.RA': 'Rings and Algebras',\n",
    "'math.RT': 'Representation Theory',\n",
    "'math.SG': 'Symplectic Geometry',\n",
    "'math.SP': 'Spectral Theory',\n",
    "'math.ST': 'Statistics Theory',\n",
    "'math-ph': 'Mathematical Physics',\n",
    "'nlin.AO': 'Adaptation and Self-Organizing Systems',\n",
    "'nlin.CD': 'Chaotic Dynamics',\n",
    "'nlin.CG': 'Cellular Automata and Lattice Gases',\n",
    "'nlin.PS': 'Pattern Formation and Solitons',\n",
    "'nlin.SI': 'Exactly Solvable and Integrable Systems',\n",
    "'nucl-ex': 'Nuclear Experiment',\n",
    "'nucl-th': 'Nuclear Theory',\n",
    "'physics.acc-ph': 'Accelerator Physics',\n",
    "'physics.ao-ph': 'Atmospheric and Oceanic Physics',\n",
    "'physics.app-ph': 'Applied Physics',\n",
    "'physics.atm-clus': 'Atomic and Molecular Clusters',\n",
    "'physics.atom-ph': 'Atomic Physics',\n",
    "'physics.bio-ph': 'Biological Physics',\n",
    "'physics.chem-ph': 'Chemical Physics',\n",
    "'physics.class-ph': 'Classical Physics',\n",
    "'physics.comp-ph': 'Computational Physics',\n",
    "'physics.data-an': 'Data Analysis, Statistics and Probability',\n",
    "'physics.ed-ph': 'Physics Education',\n",
    "'physics.flu-dyn': 'Fluid Dynamics',\n",
    "'physics.gen-ph': 'General Physics',\n",
    "'physics.geo-ph': 'Geophysics',\n",
    "'physics.hist-ph': 'History and Philosophy of Physics',\n",
    "'physics.ins-det': 'Instrumentation and Detectors',\n",
    "'physics.med-ph': 'Medical Physics',\n",
    "'physics.optics': 'Optics',\n",
    "'physics.plasm-ph': 'Plasma Physics',\n",
    "'physics.pop-ph': 'Popular Physics',\n",
    "'physics.soc-ph': 'Physics and Society',\n",
    "'physics.space-ph': 'Space Physics',\n",
    "'q-bio.BM': 'Biomolecules',\n",
    "'q-bio.CB': 'Cell Behavior',\n",
    "'q-bio.GN': 'Genomics',\n",
    "'q-bio.MN': 'Molecular Networks',\n",
    "'q-bio.NC': 'Neurons and Cognition',\n",
    "'q-bio.OT': 'Other Quantitative Biology',\n",
    "'q-bio.PE': 'Populations and Evolution',\n",
    "'q-bio.QM': 'Quantitative Methods',\n",
    "'q-bio.SC': 'Subcellular Processes',\n",
    "'q-bio.TO': 'Tissues and Organs',\n",
    "'q-fin.CP': 'Computational Finance',\n",
    "'q-fin.EC': 'Economics',\n",
    "'q-fin.GN': 'General Finance',\n",
    "'q-fin.MF': 'Mathematical Finance',\n",
    "'q-fin.PM': 'Portfolio Management',\n",
    "'q-fin.PR': 'Pricing of Securities',\n",
    "'q-fin.RM': 'Risk Management',\n",
    "'q-fin.ST': 'Statistical Finance',\n",
    "'q-fin.TR': 'Trading and Market Microstructure',\n",
    "'quant-ph': 'Quantum Physics',\n",
    "'stat.AP': 'Applications',\n",
    "'stat.CO': 'Computation',\n",
    "'stat.ME': 'Methodology',\n",
    "'stat.ML': 'Machine Learning',\n",
    "'stat.OT': 'Other Statistics',\n",
    "'stat.TH': 'Statistics Theory'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the distribution of categories\n",
    "- the categories are mostly 1 to 2 per paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_length = arxiv_dataset.select(size(split(arxiv_dataset.categories, \" \")).alias(\"categories_length\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print max, min, and average number of categories\n",
    "maxim = categories_length.agg({\"categories_length\": \"max\"}).collect()[0][0]\n",
    "minim = categories_length.agg({\"categories_length\": \"min\"}).collect()[0][0]\n",
    "avg = categories_length.agg({\"categories_length\": \"mean\"}).collect()[0][0]\n",
    "std = categories_length.agg({\"categories_length\": \"std\"}).collect()[0][0]\n",
    "median = categories_length.agg({\"categories_length\": \"median\"}).collect()[0][0]\n",
    "\n",
    "print(\"Max number of categories: \", maxim)\n",
    "print(\"Min number of categories: \", minim)\n",
    "print(\"Average number of categories: \", avg)\n",
    "print(\"Standard deviation of categories: \", std)\n",
    "print(\"Median number of categories: \", median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_length_list  = map(lambda row: row.categories_length, categories_length.collect())\n",
    "plt.hist(list(categories_length_list), bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most/Least popular category "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most frequently occuring category\n",
    "categories = arxiv_dataset.select(\"categories\").collect()\n",
    "categories = [row.categories for row in categories]\n",
    "categories = [cat.split(\" \") for cat in categories]\n",
    "categories = [item for sublist in categories for item in sublist]\n",
    "\n",
    "# most frequently occuring category\n",
    "print( \n",
    "    \"Most frequently occuring category: \", \n",
    "    max(set(categories), key = categories.count), \n",
    "    \" (\", categories.count(max(set(categories), key = categories.count)), \" times)\"\n",
    ")\n",
    "\n",
    "# least frequently occuring category\n",
    "print( \n",
    "    \"Least frequently occuring category: \", \n",
    "    min(set(categories), key = categories.count), \n",
    "    \" (\", categories.count(min(set(categories), key = categories.count)), \" times)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing the abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstract length (histogram)\n",
    "abstract_lengths = arxiv_dataset.select(size(split(\"abstract\", \" \")).alias('abstract_lengths'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = abstract_lengths.agg({\"abstract_lengths\": \"max\"}).collect()[0][0]\n",
    "min_length = abstract_lengths.agg({\"abstract_lengths\": \"min\"}).collect()[0][0]\n",
    "avg_length = abstract_lengths.agg({\"abstract_lengths\": \"mean\"}).collect()[0][0]\n",
    "std_length = abstract_lengths.agg({\"abstract_lengths\": \"std\"}).collect()[0][0]\n",
    "median_length = abstract_lengths.agg({\"abstract_lengths\": \"median\"}).collect()[0][0]\n",
    "\n",
    "# In number of words\n",
    "print(\"Max length of abstract: \", max_length)\n",
    "print(\"Min length of abstract: \", min_length)\n",
    "print(\"Average length of abstract: \", avg_length)\n",
    "print(\"Standard deviation of abstract length: \", std_length)\n",
    "print(\"Median length of abstract: \", median_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_lengths_list = map(lambda row: row.abstract_lengths, abstract_lengths.collect())\n",
    "\n",
    "plt.hist(list(abstract_lengths_list), bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after some research : spacy is a bit faster than nltk on large datasets\n",
    "def lemmatization(text):\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split(\" \")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=StringType())\n",
    "def lemmatization_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=StringType())\n",
    "def remove_stopwords(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.text for token in doc if not token.is_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=StringType())\n",
    "def remove_punctuation(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.text for token in doc if not token.is_punct])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=StringType())\n",
    "def convert_to_lowercase(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear abstract column\n",
    "arxiv_dataset = remove_empty_fields(arxiv_dataset, \"abstract\")\n",
    "arxiv_dataset = remove_empty_newlines(arxiv_dataset, \"abstract\")\n",
    "arxiv_dataset = remove_math_formula(arxiv_dataset, \"abstract\")\n",
    "arxiv_dataset = remove_extra_spaces(arxiv_dataset, \"abstract\")\n",
    "# convert to lowercase\n",
    "arxiv_dataset = arxiv_dataset.withColumn(\"abstract\", convert_to_lowercase(\"abstract\"))\n",
    "# remove punctuation\n",
    "arxiv_dataset = arxiv_dataset.withColumn(\"abstract\", remove_punctuation(\"abstract\"))\n",
    "# remove stopwords\n",
    "arxiv_dataset = arxiv_dataset.withColumn(\"abstract\", remove_stopwords(\"abstract\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_dataset = arxiv_dataset.withColumn(\"abstract\", lemmatization_spacy(\"abstract\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = arxiv_dataset.select(\"abstract\").rdd.map(lambda x: x.abstract.split()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(abstracts)\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in abstracts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = models.LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=10, random_state=42, passes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [\n",
    "    lda_model.get_document_topics(c)\n",
    "    for c in corpus\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_probable_topic(t):\n",
    "    t = np.array(t)\n",
    "    idx = t[:, 1].argmax()\n",
    "    return int(t[idx, 0])\n",
    "\n",
    "final_topics = [most_probable_topic(t) for t in topics]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query for similar articles (based on the same topic/category/common authors/other criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading back the data\n",
    "arxiv_dataset = load_data_from_json(sc, \"arxiv-sample.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"abstract\", outputCol=\"words\")\n",
    "words_data = tokenizer.transform(arxiv_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashing_tf = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=1000)\n",
    "featurized_data = hashing_tf.transform(words_data)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idf_model = idf.fit(featurized_data)\n",
    "tfidf_data = idf_model.transform(featurized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate TF-IDF similarity between articles\n",
    "article_id = \"0704.0001\"  # Example article ID\n",
    "article_tfidf = tfidf_data.filter(col(\"id\") == article_id).select(\"features\").first()[\"features\"]\n",
    "article_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_id2 = \"0704.0002\"  # Example article ID\n",
    "article_tfidf2 = tfidf_data.filter(col(\"id\") == article_id2).select(\"features\").first()[\"features\"]\n",
    "article_tfidf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return float(a.dot(b) / (a.norm(2) * b.norm(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cosine similarity between article_id and other articles and sort them\n",
    "similarity = tfidf_data.rdd.map(lambda x: (x.id, cosine_similarity(article_tfidf, x.features))).sortBy(lambda x: x[1], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the second article from similarity list\n",
    "article_id2 = similarity.take(2)[1][0]\n",
    "article_id2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the information of the second article\n",
    "arxiv_dataset.filter(arxiv_dataset.id == article_id2).select(\"title\", \"abstract\", \"authors\", \"categories\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the information of the first article\n",
    "arxiv_dataset.filter(arxiv_dataset.id == article_id).select(\"title\", \"abstract\", \"authors\", \"categories\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### query for similar articles based on category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Similar article based on category\n",
    "def get_similar_articles_by_category(article_id):\n",
    "    # iterate over all articles \n",
    "    categories_search = (\n",
    "        arxiv_dataset.filter(arxiv_dataset.id == article_id)\n",
    "        .select(\"categories\")\n",
    "        .rdd.map(lambda x: x.categories)\n",
    "        .map(lambda x: x.split(\" \"))\n",
    "        .map(lambda x: [item for item in x])\n",
    "        .flatMap(lambda x: x)\n",
    "        .collect()\n",
    "    )\n",
    "    include_all = len(categories_search)\n",
    "    if include_all == 0: # invalid article id\n",
    "        return []\n",
    "    similar_categories = (\n",
    "        arxiv_dataset.rdd.map(lambda x: (x.id, x.categories))\n",
    "        .map(lambda x: (x[0], x[1].split(\" \")))\n",
    "        .filter(lambda x: len(list(value for value in x[1] if value in categories_search)) == include_all)\n",
    "        .map(lambda x: x[0])\n",
    "        .filter(lambda x: x != article_id)\n",
    "        .collect()\n",
    "    )\n",
    "    return similar_categories\n",
    "\n",
    "# get a random article from the list\n",
    "article_ = arxiv_dataset.rdd.takeSample(False, 1)[0]\n",
    "# show the article category\n",
    "print(\"categories to search after:\",  article_.categories)\n",
    "similar_articles = get_similar_articles_by_category(article_.id)\n",
    "if len(similar_articles) > 0:\n",
    "    test_id = similar_articles[0]\n",
    "    arxiv_dataset.filter(arxiv_dataset.id == test_id).select(\"title\", \"abstract\", \"authors\", \"categories\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### query for similar articles based on common authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_articles_by_common_authors(article_id):\n",
    "    # get list of authors for the article\n",
    "    authors_search = (\n",
    "        arxiv_dataset.filter(arxiv_dataset.id == article_id)\n",
    "        .select(\"authors_list\")\n",
    "        .rdd.map(lambda x: x.authors_list)\n",
    "        .map(lambda x: [item for item in x])\n",
    "        .flatMap(lambda x: x)\n",
    "        .collect()\n",
    "    )\n",
    "    similar_authors = (\n",
    "        arxiv_dataset.rdd.map(lambda x: (x.id, x.authors_list))\n",
    "        .map(lambda x: (x[0], x[1]))\n",
    "        .filter(lambda x: len(list(value for value in x[1] if value in authors_search)) > 0)\n",
    "        .map(lambda x: x[0])\n",
    "        .filter(lambda x: x != article_id)\n",
    "        .collect()\n",
    "    )\n",
    "    return similar_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess dataset authors column\n",
    "arxiv_dataset = arxiv_dataset.withColumn(\"authors_list\", get_authors_list(\"authors_parsed\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a random article from the list\n",
    "article_ = arxiv_dataset.rdd.takeSample(False, 1)[0]\n",
    "\n",
    "authors_to_search = article_.authors_list\n",
    "\n",
    "print(\"authors to search after:\", authors_to_search)\n",
    "similar_articles = get_similar_articles_by_common_authors(article_.id)\n",
    "\n",
    "if len(similar_articles) > 0:\n",
    "    test_id = similar_articles[0]\n",
    "    arxiv_dataset.filter(arxiv_dataset.id == test_id).select(\"title\", \"abstract\", \"authors\", \"categories\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification (for a new article to determine its category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First approach:\n",
    "- use TF-IDF to vectorize the abstracts and Tokeniezer to tokenize the abstracts\n",
    "- use CountVectors (\"document-term vectors\") and regex tokenizer to tokenize the abstracts ( + remove stop words)\n",
    "\n",
    "Models to use:\n",
    "- Naive Bayes\n",
    "- Logistic Regression\n",
    "- Random Forest\n",
    "\n",
    "Finally: use Cross-Validation -> try to tune the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading back the data\n",
    "arxiv_dataset = load_data_from_json(sc, \"arxiv-sample.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns\n",
    "drop_colums = [\"authors_parsed\", \"authors_list\", \"comments\", \"doi\", \"journal-ref\", \"license\", \"report-no\", \"submitter\", \"title\", \"versions\", \"authors\", \"update_date\"]\n",
    "arxiv_dataset = arxiv_dataset.drop(*drop_colums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the categories column, get the first category\n",
    "arxiv_dataset = arxiv_dataset.withColumn(\"category\", split(\"categories\", \" \")[0])\n",
    "# drop the categories column\n",
    "arxiv_dataset = arxiv_dataset.drop(\"categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_indexer = StringIndexer(inputCol=\"category\", outputCol=\"label\", handleInvalid=\"skip\")\n",
    "indexer_fitted = string_indexer.fit(arxiv_dataset)\n",
    "labels = indexer_fitted.labels # retrieve labels in order to use them later\n",
    "arxiv_dataset = indexer_fitted.transform(arxiv_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "arxiv_dataset = arxiv_dataset.withColumn(\"abstract\", remove_stopwords(\"abstract\"))\n",
    "# convert to lowercase\n",
    "arxiv_dataset = arxiv_dataset.withColumn(\"abstract\", convert_to_lowercase(\"abstract\"))\n",
    "# remove punctuation\n",
    "arxiv_dataset = arxiv_dataset.withColumn(\"abstract\", remove_punctuation(\"abstract\"))\n",
    "# apply lemmatization\n",
    "arxiv_dataset = arxiv_dataset.withColumn(\"abstract\", lemmatization_spacy(\"abstract\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the models: Logistic Regression, Random Forest, Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=30, regParam=0.3, elasticNetParam=0)\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\n",
    "nb = NaiveBayes(labelCol=\"label\", featuresCol=\"features\", smoothing=1.0, modelType=\"multinomial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "desired approach: prepare the training data and then use what model you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a pipeline with stages for tokenization, TF-IDF conversion, and Logistic Regression\n",
    "tokenizer = Tokenizer(inputCol=\"abstract\", outputCol=\"words\")\n",
    "hashing_tf = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "\n",
    "# minDocFreq: remove sparse terms\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5)\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, hashing_tf, idf])\n",
    "\n",
    "# fit the pipeline\n",
    "pipeline_model = pipeline.fit(arxiv_dataset)\n",
    "dataset = pipeline_model.transform(arxiv_dataset)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "(training_data, testing_data) = dataset.randomSplit([0.8, 0.2], seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = lr.fit(training_data)\n",
    "model_rf = rf.fit(training_data)\n",
    "model_nb = nb.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the testing data\n",
    "predictions_lr = model_lr.transform(testing_data)\n",
    "predictions_rf = model_rf.transform(testing_data)\n",
    "predictions_nb = model_nb.transform(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show predictions\n",
    "predictions_lr.select(\"abstract\", \"label\", \"prediction\").show(5)\n",
    "predictions_rf.select(\"abstract\", \"label\", \"prediction\").show(5)\n",
    "predictions_nb.select(\"abstract\", \"label\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "accuracy_lr = evaluator.evaluate(predictions_lr)\n",
    "accuracy_rf = evaluator.evaluate(predictions_rf)\n",
    "accuracy_nb = evaluator.evaluate(predictions_nb)\n",
    "\n",
    "print(\"Accuracy of Logistic Regression: \", accuracy_lr)\n",
    "print(\"Accuracy of Random Forest: \", accuracy_rf)\n",
    "print(\"Accuracy of Naive Bayes: \", accuracy_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use CountVectorizer instead of HashingTF and regexTokenizer instead of Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading back the data\n",
    "arxiv_dataset = load_data_from_json(sc, \"arxiv-sample.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns\n",
    "drop_colums = [\"authors_parsed\", \"authors_list\", \"comments\", \"doi\", \"journal-ref\", \"license\", \"report-no\", \"submitter\", \"title\", \"versions\", \"authors\", \"update_date\"]\n",
    "arxiv_dataset = arxiv_dataset.drop(*drop_colums)\n",
    "# from the categories column, get the first category\n",
    "arxiv_dataset = arxiv_dataset.withColumn(\"category\", split(\"categories\", \" \")[0])\n",
    "# drop the categories column\n",
    "arxiv_dataset = arxiv_dataset.drop(\"categories\")\n",
    "string_indexer = StringIndexer(inputCol=\"category\", outputCol=\"label\", handleInvalid=\"skip\")\n",
    "indexer_fitted = string_indexer.fit(arxiv_dataset)\n",
    "labels = indexer_fitted.labels # retrieve labels in order to use them later\n",
    "arxiv_dataset = indexer_fitted.transform(arxiv_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexTokenizer = RegexTokenizer(inputCol=\"abstract\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)\n",
    "\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors])\n",
    "\n",
    "# fit the pipeline\n",
    "pipeline_fit = pipeline.fit(arxiv_dataset)\n",
    "dataset = pipeline_fit.transform(arxiv_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "(training_data, testing_data) = dataset.randomSplit([0.8, 0.2], seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model_lr = lr.fit(training_data)\n",
    "model_rf = rf.fit(training_data)\n",
    "model_nb = nb.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the testing data\n",
    "predictions_lr = model_lr.transform(testing_data)\n",
    "predictions_rf = model_rf.transform(testing_data)\n",
    "predictions_nb = model_nb.transform(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy_lr = evaluator.evaluate(predictions_lr)\n",
    "accuracy_rf = evaluator.evaluate(predictions_rf)\n",
    "accuracy_nb = evaluator.evaluate(predictions_nb)\n",
    "\n",
    "print(\"Accuracy of Logistic Regression: \", accuracy_lr)\n",
    "print(\"Accuracy of Random Forest: \", accuracy_rf)\n",
    "print(\"Accuracy of Naive Bayes: \", accuracy_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on a single abstract (new article)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on a single abstract from the testing data\n",
    "prediction = model.transform(testing_data.filter(testing_data.id == \"0704.1072\"))\n",
    "\n",
    "# take the probability array of the prediction\n",
    "prediction.select(\"probability\").show(truncate=False)\n",
    "# convert the probability array to a list\n",
    "list_prediction = prediction.select(\"probability\").collect()[0][0].toArray().tolist()\n",
    "print(list_prediction)\n",
    "threshold = 0.2\n",
    "for i in range(len(list_prediction)):\n",
    "    if list_prediction[i] > threshold:\n",
    "        print(labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Articles Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_category(dataset, category):\n",
    "  return dataset.filter(dataset.categories.contains(category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data(rfc_1123_date):\n",
    "  return datetime.strftime(datetime.strptime(rfc_1123_date, \"%a, %d %b %Y %H:%M:%S %Z\"), \"%Y-%m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(dataset, category):\n",
    "  dataset_panda = dataset.select(\"createDate\", \"count\").toPandas()\n",
    "\n",
    "  plt.plot(dataset_panda[\"createDate\"], dataset_panda[\"count\"])\n",
    "  plt.xlabel('Date')\n",
    "  plt.ylabel('Count')\n",
    "  plt.xticks(rotation=60, ha='right')\n",
    "  plt.title('Paper over time for {} category'.format(category))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_papers_over_time(category, df):\n",
    "  # extract creation date\n",
    "  convert_data_f = udf(convert_data, StringType())\n",
    "  modified_df = df.withColumn('createDate', convert_data_f(df.versions[0]['created']))\n",
    "\n",
    "  modified_df = filter_by_category(modified_df, category)\n",
    "  modified_df = modified_df.groupBy('createDate').count().orderBy(asc('createDate'))\n",
    "  create_graph(modified_df, category)\n",
    "\n",
    "show_papers_over_time('hep-th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_papers_over_time(category, df):\n",
    "  # extract creation date\n",
    "  convert_data_f = udf(convert_data, StringType())\n",
    "  df = df.withColumn('createDate', convert_data_f(df.versions[0]['created']))\n",
    "\n",
    "  df = filter_by_category(df, category)\n",
    "  df = df.groupBy('createDate').count().orderBy(asc('createDate'))\n",
    "  create_graph(df, category)\n",
    "\n",
    "show_papers_over_time('hep-th')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster and top words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset, input_col, output_col):\n",
    "  tokenizer = Tokenizer(inputCol=input_col, outputCol=output_col)\n",
    "  return tokenizer.transform(dataset)\n",
    "\n",
    "def vectorize_dataset(dataset, input_col, output_col):\n",
    "  dataset = remove_extra_spaces(dataset, \"categories\")\n",
    "  words2vec_model = Word2Vec(inputCol=\"words\",\n",
    "                             outputCol=output_col,\n",
    "                             vectorSize=100,\n",
    "                             minCount=1)\n",
    "  return words2vec_model.fit(tokenize_dataset(dataset, input_col, \"words\"))\n",
    "\n",
    "def prepare_data(dataset, output_col='scaled_categories', input_col=\"categories\"):\n",
    "  scaler = StandardScaler(inputCol=\"vector\",\n",
    "                          outputCol=output_col,\n",
    "                          withStd=True,\n",
    "                          withMean=False)\n",
    "  model = vectorize_dataset(dataset, input_col, \"vector\")\n",
    "  scalerModel = scaler.fit(model.getVectors())\n",
    "  # data.select('scaled_categories').show(5)\n",
    "  return scalerModel.transform(model.getVectors())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_silhouette_score_graph(dataset):\n",
    "  silhouette_score = []\n",
    "  evaluator = ClusteringEvaluator(predictionCol=\"prediction\",\n",
    "                                  featuresCol='scaled_categories',\n",
    "                                  metricName='silhouette',\n",
    "                                  distanceMeasure='squaredEuclidean')\n",
    "  dataset = prepare_data(dataset)\n",
    "  for i in range(2,10):\n",
    "    kmeans=KMeans(featuresCol='scaled_categories', k=i)\n",
    "    model=kmeans.fit(dataset)\n",
    "    predictions=model.transform(dataset)\n",
    "    score=evaluator.evaluate(predictions)\n",
    "    silhouette_score.append(score)\n",
    "    print('Silhouette Score for k =',i,'is',score)\n",
    "\n",
    "  # Choose k = 4\n",
    "  plt.plot(range(2,10),silhouette_score)\n",
    "  plt.xlabel('k')\n",
    "  plt.ylabel('silhouette score')\n",
    "  plt.title('Silhouette Score')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clusters(dataset, k_value):\n",
    "  kmeans = KMeans(featuresCol='scaled_categories', k=k_value)\n",
    "  return kmeans.fit(prepare_data(dataset))\n",
    "\n",
    "def create_predictions(model, dataset):\n",
    "  return model.transform(prepare_data(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "  sc = init_spark()[0]\n",
    "  arxiv_dataset = load_data_from_json(sc, \"part1.json\")\n",
    "\n",
    "  show_silhouette_score_graph(arxiv_dataset)\n",
    "\n",
    "  model = create_clusters(arxiv_dataset, 4)\n",
    "  print(\"Cluster Centers: \")\n",
    "  for center in model.clusterCenters():\n",
    "    print(center)\n",
    "\n",
    "  create_predictions(model, arxiv_dataset).select('prediction').show(10)\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset):\n",
    "  dataset = remove_empty_fields(dataset, \"abstract\")\n",
    "  dataset = remove_empty_newlines(dataset, \"abstract\")\n",
    "  dataset = remove_math_formula(dataset, \"abstract\")\n",
    "  dataset = convert_to_lowercase(dataset, \"abstract\")\n",
    "  dataset = remove_extra_spaces(dataset, \"abstract\")\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_word_cloud(dataset, category):\n",
    "  text =  \" \".join(dataset.filter(dataset.categories.contains(category)).select(\"abstract\").rdd.flatMap(lambda x: x).collect())\n",
    "  wordcloud = WordCloud().generate(text)\n",
    "\n",
    "  plt.imshow(wordcloud, interpolation='bilinear')\n",
    "  plt.axis(\"off\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_word_top(dataset, category, count):\n",
    "  dataset = remove_stop_words(dataset, \"abstract\", \"filtered_abstract\", list(STOPWORDS))\n",
    "  dataset = dataset.filter(dataset.categories.contains(category))\n",
    "  dataset = dataset.withColumn('filtered_abstract', explode(split('filtered_abstract', ' '))).groupBy('filtered_abstract').count().orderBy(desc('count'))\n",
    "  top_val_dict = {r['filtered_abstract']:r['count'] for r in dataset.head(count)}\n",
    "  top_val_dict.pop(\"\")\n",
    "  print(top_val_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "  arxiv_dataset = load_data_from_json(sc, \"arxiv-sample.json\")\n",
    "  show_word_cloud(arxiv_dataset, \"stat.TH\")\n",
    "  show_word_top(arxiv_dataset, \"stat.TH\", 10)\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Prolific authors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Article categories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_df = arxiv_dataset.select(\"categories\")\n",
    "distinct_categories_df = categories_df.distinct()\n",
    "categories_list = distinct_categories_df.rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "for i in range(len(categories_list)):\n",
    "    element = categories_list[i]\n",
    "    categories = element.split()\n",
    "    categories_list[i] = categories\n",
    "\n",
    "categories_list = set([item for sublist in categories_list for item in sublist])\n",
    "\n",
    "print(categories_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Articles authors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_df = arxiv_dataset.select(\"authors\")\n",
    "distinct_authors_df = authors_df.distinct()\n",
    "authors_list = distinct_authors_df.rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "authors_list = remove_text_between_parentheses(authors_list)\n",
    "authors_list = remove_text_between_parentheses(authors_list)\n",
    "\n",
    "authors_list = remove_substring_from_list(authors_list, \"et al\")\n",
    "authors_list = remove_characters_before_substring(authors_list, \"Collaboration:\")\n",
    "\n",
    "authors_list = split_function(\",\", authors_list)\n",
    "authors_list = split_function(\" and \", authors_list)\n",
    "\n",
    "authors_list = delete_new_line_and_spaces(authors_list)\n",
    "authors_list = delete_duplicate(authors_list)\n",
    "authors_list = delete_letter(authors_list)\n",
    "authors_list = delete_element(authors_list, \"Collaboration\")\n",
    "\n",
    "print(authors_list)\n",
    "print(len(authors_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Find prolific authors for a specific category**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categories_list:\n",
    "  filtered_df = arxiv_dataset.filter(col(\"categories\").like(f\"%{category}%\"))\n",
    "  authors_per_cathegory = filtered_df.select(\"authors\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "  profilic_authors = {}\n",
    "  for author in authors_per_cathegory:\n",
    "      for author_name in authors_list:\n",
    "          if author_name.lower() in author.lower():\n",
    "              if author_name in profilic_authors:\n",
    "                  profilic_authors[author_name] += 1\n",
    "              else:\n",
    "                  profilic_authors[author_name] = 1\n",
    "  max_author = max(profilic_authors, key=profilic_authors.get)\n",
    "  print(\"Category: \" + category + \" Author: \" + str(max_author))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
